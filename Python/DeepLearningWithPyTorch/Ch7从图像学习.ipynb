{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 微小图像数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 下载CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets\n",
    "data_path = 'D:/Data/CIFAR10/'\n",
    "cifar10 = datasets.CIFAR10(data_path, train=True, download=True)\n",
    "cifar10_val = datasets.CIFAR10(data_path, train=False, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">  使用train=False,获取一个数据集用于验证数据，并在需要时再次下载该数据集<br> 我们提供给CIFAR10()函数的第1个参数是数据的下载位置；第2个参数指定我们对训练集还是验证集感兴趣；第3个参数表示如果在第1个参数指定的位置找不到数据，我们是否运行PyTorch下载数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 数据集子模块为我们提供了对最流行的计算机视觉数据集的预储存访问，如MNIST、Fashion-MNIST、CIFAR-10、XVHN、COCO和Omniglot等。在每种情况下，数据集都作为torch.utils.data.Dataset的子类返回"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torchvision.datasets.cifar.CIFAR10,\n",
       " torchvision.datasets.vision.VisionDataset,\n",
       " torch.utils.data.dataset.Dataset,\n",
       " typing.Generic,\n",
       " object)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(cifar10).__mro__ # 实例cifar10的方法解析顺序中包含了它的父类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Dataset对象不一定持有数据，但是提供了对其进行统一访问的函数__len__()和__getitem__()，前者返回数据中的项数，后者返回由样本和与之对应的标签（整数索引）组成的项"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 当Python对象配备了__len__()函数是，可以将其作为参数传递给Python的内置函数len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cifar10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 由于数据集配备了__getitem__()函数，可以使用标准索引对元组和列表进行索引，以访问单个数据项"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PIL.Image.Image image mode=RGB size=32x32 at 0x11A692D9730>, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img, label = cifar10[99]\n",
    "img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset变换"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 引入torchvision.transforms模块将PIL图像变换为PyTorch张量，这个模块定义了一组和组合的、类似函数的对象，它可以作为参数传递到TorchVision模块的数据集，诸如datasets.CIFAR10(...)，在数据加载之后，在__getitem__()返回之前对数据进行变换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AutoAugment',\n",
       " 'AutoAugmentPolicy',\n",
       " 'CenterCrop',\n",
       " 'ColorJitter',\n",
       " 'Compose',\n",
       " 'ConvertImageDtype',\n",
       " 'FiveCrop',\n",
       " 'GaussianBlur',\n",
       " 'Grayscale',\n",
       " 'InterpolationMode',\n",
       " 'Lambda',\n",
       " 'LinearTransformation',\n",
       " 'Normalize',\n",
       " 'PILToTensor',\n",
       " 'Pad',\n",
       " 'RandAugment',\n",
       " 'RandomAdjustSharpness',\n",
       " 'RandomAffine',\n",
       " 'RandomApply',\n",
       " 'RandomAutocontrast',\n",
       " 'RandomChoice',\n",
       " 'RandomCrop',\n",
       " 'RandomEqualize',\n",
       " 'RandomErasing',\n",
       " 'RandomGrayscale',\n",
       " 'RandomHorizontalFlip',\n",
       " 'RandomInvert',\n",
       " 'RandomOrder',\n",
       " 'RandomPerspective',\n",
       " 'RandomPosterize',\n",
       " 'RandomResizedCrop',\n",
       " 'RandomRotation',\n",
       " 'RandomSolarize',\n",
       " 'RandomVerticalFlip',\n",
       " 'Resize',\n",
       " 'TenCrop',\n",
       " 'ToPILImage',\n",
       " 'ToTensor',\n",
       " 'TrivialAugmentWide',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " 'autoaugment',\n",
       " 'functional',\n",
       " 'functional_pil',\n",
       " 'functional_tensor',\n",
       " 'transforms']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "dir(transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 在这些变换对象中，可以看到ToTensor对象，它将NumPy数组和PIL图像变换为张量，还将输出张量的尺寸设置为$C\\times H\\times W$(通道、高度、宽度）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 32])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "to_tensor = transforms.ToTensor()\n",
    "img_t = to_tensor(img)\n",
    "img_t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 可以将变换直接作为参数传递给dataset.CIFAR10,此时，访问数据集的元素将返回一个张量，而不是PIL图像"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_cifar10 = datasets.CIFAR10(data_path, train=True, download=False, transform=transforms.ToTensor())\n",
    "img_t, _ = tensor_cifar10[99]\n",
    "type(img_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 32, 32]), torch.float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_t.shape, img_t.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.), tensor(1.))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_t.min(), img_t.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(img_t.permute(1, 2, 0)) # 使用permute将轴的顺序由C*H*W改为H*W*C，因为matplotlib的期望输出是H*W*C\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据归一化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 32, 50000])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "# 将数据集返回的所有张量（所有图像）沿着一个额外的维度进行堆叠\n",
    "imgs = torch.stack([img_t for img_t, _ in tensor_cifar10], dim=3)\n",
    "imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4914, 0.4822, 0.4465])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs.view(3, -1).mean(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> view(3, -1)保留了3个通道（第1个维度，C维度），并将剩余的所有维度合并为一个维度，从而计算出适当的尺寸大小。3*32*32的图像被转换成3*1024的向量，然后对每个通道的1024个元素取平均值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2470, 0.2435, 0.2616])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs.view(3, -1).std(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.247, 0.2435, 0.2616))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10 = datasets.CIFAR10(data_path,\n",
    "                           train=True,\n",
    "                           download=False,\n",
    "                           transform=transforms.Compose([\n",
    "                               transforms.ToTensor(), # 如果没有这句在DataLoader的时候会出错\n",
    "                               transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                                                    (0.2470, 0.2435, 0.2616))\n",
    "                           ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10_val = datasets.CIFAR10(\n",
    "    data_path, train=False, download=False,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
    "                             (0.2470, 0.2435, 0.2616))\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 此时从数据集绘制的图像不能为我们提供实际图像的真实表示\n",
    "img_t, _  = cifar10[99]\n",
    "plt.imshow(img_t.permute(1, 2, 0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 建立模型（区分飞鸟和飞机）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {0:0, 2:1}\n",
    "class_names = ['airplane', 'bird']\n",
    "cifar2 = [(img, label_map[label])\n",
    "         for img, label in cifar10\n",
    "         if label in [0, 2]]\n",
    "cifar2_val = [(img, label_map[label])\n",
    "             for img, label in cifar10_val\n",
    "             if label in [0, 2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用softmax()输出概率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> nn模块将Softmax作为一个可用的模块<br> nn.Softmax()要求我们指定用来编码概率的维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0900, 0.2447, 0.6652],\n",
       "        [0.0900, 0.2447, 0.6652]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "softmax = nn.Softmax(dim=1)\n",
    "\n",
    "# 示例\n",
    "x = torch.tensor([[1.0, 2.0, 3.0],\n",
    "                  [1.0, 2.0, 3.0]])\n",
    "softmax(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建全连接模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 每个样本有32*32*3个特征\n",
    "# 构建一个线性模型，有3072个输入特征和一些隐藏特征，接着是一个激活函数，然后另一个线性模型将使网络的输出特征逐渐减少到适当的数量，最后添加一个softmax()函数输出概率\n",
    "model = nn.Sequential(nn.Linear(3072, 512),\n",
    "                      nn.Tanh(),\n",
    "                      nn.Linear(512, 2),\n",
    "                      nn.Softmax(dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 负对数似然（NLL）损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 我们想要一个损失函数，当概率很低时，损失非常高：低到其他选择都有比它更高的概率。相反，当概率远高于其他选择时，损失应该很低<br> 并不是要真的专注于将概率提高到1，即使此时仍然可以使用MSE使输出概率收敛到[0,1]和[1,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **负对数似然（NLL）**可以作为预测概率的函数，当预测目标类别的概率较低时，NLL会增长到无穷大，二挡预测目标类别概率大于0.5时，NLL的下降速度非常慢，NLL把概率作为输入<br> PyTorch有一个nn.NLLLoss类，没有取概率而是取对数概率张量作为输入，所以将模型输出改为nn.LogSoftmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(3072, 512),\n",
    "                      nn.Tanh(),\n",
    "                      nn.Linear(512, 2),\n",
    "                      nn.LogSoftmax(dim=1))\n",
    "loss = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.内部循环，批处理样本"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 在一个批处理中评估10000幅图像太多了，所以我们决定进行一个内部循环，每次评估一个样本，然后在单个样本上反向传播<br> 但是基于一个样本的减小损失的好方向对其他样本可能不是好方向。通过在每个迭代周期上变换样本并一次估计几个样本的梯度（为了稳定性更高），我们在梯度下降中有效的引入了随机性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> torch.utils.data模块中有一个DataLoader类，该类有助于打乱数据和组织数据。数据加载器的工作是从数据集中采样小批量，这是我们能够灵活地选择不同的采样策略<br> DataLoader()构造函数至少接收一个数据集对象作为输入，以及batch_size和一个shuffle布尔值，该布尔值指示数据是否需要在每个迭代周期开始被重新打乱<br> DataLoader可以被迭代，因此可以直接在新训练代码的内部循环中使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.854905\n",
      "Epoch: 1, Loss: 0.460641\n",
      "Epoch: 2, Loss: 0.567613\n",
      "Epoch: 3, Loss: 0.488258\n",
      "Epoch: 4, Loss: 0.463740\n",
      "Epoch: 5, Loss: 0.370732\n",
      "Epoch: 6, Loss: 0.379118\n",
      "Epoch: 7, Loss: 0.323742\n",
      "Epoch: 8, Loss: 0.200625\n",
      "Epoch: 9, Loss: 0.263990\n",
      "Epoch: 10, Loss: 0.457850\n",
      "Epoch: 11, Loss: 0.221490\n",
      "Epoch: 12, Loss: 0.577265\n",
      "Epoch: 13, Loss: 0.223069\n",
      "Epoch: 14, Loss: 0.360158\n",
      "Epoch: 15, Loss: 0.385874\n",
      "Epoch: 16, Loss: 0.385812\n",
      "Epoch: 17, Loss: 0.258929\n",
      "Epoch: 18, Loss: 0.296741\n",
      "Epoch: 19, Loss: 0.296134\n",
      "Epoch: 20, Loss: 0.183760\n",
      "Epoch: 21, Loss: 0.481952\n",
      "Epoch: 22, Loss: 0.297204\n",
      "Epoch: 23, Loss: 0.139974\n",
      "Epoch: 24, Loss: 0.436681\n",
      "Epoch: 25, Loss: 0.266337\n",
      "Epoch: 26, Loss: 0.557288\n",
      "Epoch: 27, Loss: 0.284492\n",
      "Epoch: 28, Loss: 0.294213\n",
      "Epoch: 29, Loss: 0.219615\n",
      "Epoch: 30, Loss: 0.123639\n",
      "Epoch: 31, Loss: 0.130580\n",
      "Epoch: 32, Loss: 0.389572\n",
      "Epoch: 33, Loss: 0.314653\n",
      "Epoch: 34, Loss: 0.114252\n",
      "Epoch: 35, Loss: 0.050402\n",
      "Epoch: 36, Loss: 0.374908\n",
      "Epoch: 37, Loss: 0.072397\n",
      "Epoch: 38, Loss: 0.287532\n",
      "Epoch: 39, Loss: 0.131610\n",
      "Epoch: 40, Loss: 0.077340\n",
      "Epoch: 41, Loss: 0.167810\n",
      "Epoch: 42, Loss: 0.227020\n",
      "Epoch: 43, Loss: 0.140906\n",
      "Epoch: 44, Loss: 0.049424\n",
      "Epoch: 45, Loss: 0.507218\n",
      "Epoch: 46, Loss: 0.122540\n",
      "Epoch: 47, Loss: 0.065579\n",
      "Epoch: 48, Loss: 0.042083\n",
      "Epoch: 49, Loss: 0.148898\n",
      "Epoch: 50, Loss: 0.081198\n",
      "Epoch: 51, Loss: 0.033092\n",
      "Epoch: 52, Loss: 0.119779\n",
      "Epoch: 53, Loss: 0.069205\n",
      "Epoch: 54, Loss: 0.072981\n",
      "Epoch: 55, Loss: 0.122405\n",
      "Epoch: 56, Loss: 0.061773\n",
      "Epoch: 57, Loss: 0.027258\n",
      "Epoch: 58, Loss: 0.043419\n",
      "Epoch: 59, Loss: 0.050317\n",
      "Epoch: 60, Loss: 0.055163\n",
      "Epoch: 61, Loss: 0.059949\n",
      "Epoch: 62, Loss: 0.079858\n",
      "Epoch: 63, Loss: 0.039192\n",
      "Epoch: 64, Loss: 0.043426\n",
      "Epoch: 65, Loss: 0.019178\n",
      "Epoch: 66, Loss: 0.178201\n",
      "Epoch: 67, Loss: 0.118962\n",
      "Epoch: 68, Loss: 0.041174\n",
      "Epoch: 69, Loss: 0.014135\n",
      "Epoch: 70, Loss: 0.012130\n",
      "Epoch: 71, Loss: 0.059705\n",
      "Epoch: 72, Loss: 0.035136\n",
      "Epoch: 73, Loss: 0.013736\n",
      "Epoch: 74, Loss: 0.050447\n",
      "Epoch: 75, Loss: 0.023188\n",
      "Epoch: 76, Loss: 0.027897\n",
      "Epoch: 77, Loss: 0.026866\n",
      "Epoch: 78, Loss: 0.013168\n",
      "Epoch: 79, Loss: 0.020649\n",
      "Epoch: 80, Loss: 0.016914\n",
      "Epoch: 81, Loss: 0.009860\n",
      "Epoch: 82, Loss: 0.017385\n",
      "Epoch: 83, Loss: 0.038528\n",
      "Epoch: 84, Loss: 0.011242\n",
      "Epoch: 85, Loss: 0.037482\n",
      "Epoch: 86, Loss: 0.025128\n",
      "Epoch: 87, Loss: 0.038967\n",
      "Epoch: 88, Loss: 0.022287\n",
      "Epoch: 89, Loss: 0.060049\n",
      "Epoch: 90, Loss: 0.023840\n",
      "Epoch: 91, Loss: 0.014102\n",
      "Epoch: 92, Loss: 0.037956\n",
      "Epoch: 93, Loss: 0.026167\n",
      "Epoch: 94, Loss: 0.033278\n",
      "Epoch: 95, Loss: 0.061708\n",
      "Epoch: 96, Loss: 0.033915\n",
      "Epoch: 97, Loss: 0.010959\n",
      "Epoch: 98, Loss: 0.012180\n",
      "Epoch: 99, Loss: 0.006977\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=True)\n",
    "\n",
    "model = nn.Sequential(nn.Linear(3072, 512),\n",
    "                      nn.Tanh(),\n",
    "                      nn.Linear(512, 2),\n",
    "                      nn.LogSoftmax(dim=1))\n",
    "\n",
    "learning_rate = 1e-2\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_fn = nn.NLLLoss()\n",
    "\n",
    "n_epochs = 100\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for imgs, labels in train_loader:\n",
    "        batch_size = imgs.shape[0]\n",
    "        outputs = model(imgs.view(batch_size, -1))\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(\"Epoch: %d, Loss: %f\" % (epoch, float(loss)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.分类准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.818500\n"
     ]
    }
   ],
   "source": [
    "val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64, shuffle=False)\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in val_loader:\n",
    "        batch_size = imgs.shape[0]\n",
    "        outputs = model(imgs.view(batch_size, -1))\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        total += labels.shape[0]\n",
    "        correct += int((predicted == labels).sum())\n",
    "print(\"Accuracy: %f\" % (correct/total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.增加神经网络层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(3072, 1024),\n",
    "                      nn.Tanh(),\n",
    "                      nn.Linear(1024, 512),\n",
    "                      nn.Tanh(),\n",
    "                      nn.Linear(512. 128),\n",
    "                      nn.Tanh(),\n",
    "                      nn.Linear(128, 2),\n",
    "                      nn.LogSoftmax(dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.交叉熵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> nn.LogSoftmax()和nn.NLLLoss()的组合相当于使用nn.CrossEntropyLoss()<br> **交叉熵**损失函数是PyTorch的一个特性，实际上nn.NLLLoss()计算的是交叉熵，但将对数概率预测作为输入，其中nn.CrossEntropyLoss()用于计算分数（有时称为logits）<br> 从技术上讲，nn.NLLLoss()计算的是把所有质量放在目标上的狄拉克分布和由对数概率输入给出的预测分布之间的交叉熵。在信息论中，按样本大小归一化，这种交叉熵可以解释为预测分布在目标分布下的负对数似然作为结果<br> 因此，当我们的模型预测（Softmax应用）概率时，这2个损失都是给定数据的模型参数的负对数似然"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 从神经网络中丢弃最后一个nn.LogSoftmax()，转而使用nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(3072, 1024),\n",
    "                      nn.Tanh(),\n",
    "                      nn.Linear(1024, 512),\n",
    "                      nn.Tanh(),\n",
    "                      nn.Linear(512, 128),\n",
    "                      nn.Tanh(),\n",
    "                      nn.Linear(128, 2))\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.453518\n",
      "Epoch: 1, Loss: 0.611415\n",
      "Epoch: 2, Loss: 0.457055\n",
      "Epoch: 3, Loss: 0.422298\n",
      "Epoch: 4, Loss: 0.335312\n",
      "Epoch: 5, Loss: 0.563283\n",
      "Epoch: 6, Loss: 0.416869\n",
      "Epoch: 7, Loss: 0.371373\n",
      "Epoch: 8, Loss: 0.260455\n",
      "Epoch: 9, Loss: 0.297829\n",
      "Epoch: 10, Loss: 0.521220\n",
      "Epoch: 11, Loss: 0.349487\n",
      "Epoch: 12, Loss: 0.341801\n",
      "Epoch: 13, Loss: 0.374785\n",
      "Epoch: 14, Loss: 0.174531\n",
      "Epoch: 15, Loss: 0.434190\n",
      "Epoch: 16, Loss: 0.328784\n",
      "Epoch: 17, Loss: 0.324447\n",
      "Epoch: 18, Loss: 0.260563\n",
      "Epoch: 19, Loss: 0.370787\n",
      "Epoch: 20, Loss: 0.339474\n",
      "Epoch: 21, Loss: 0.337969\n",
      "Epoch: 22, Loss: 0.122827\n",
      "Epoch: 23, Loss: 0.252425\n",
      "Epoch: 24, Loss: 0.421682\n",
      "Epoch: 25, Loss: 0.105909\n",
      "Epoch: 26, Loss: 0.129121\n",
      "Epoch: 27, Loss: 0.176902\n",
      "Epoch: 28, Loss: 0.439743\n",
      "Epoch: 29, Loss: 0.162713\n",
      "Epoch: 30, Loss: 0.077676\n",
      "Epoch: 31, Loss: 0.129421\n",
      "Epoch: 32, Loss: 0.207449\n",
      "Epoch: 33, Loss: 0.024648\n",
      "Epoch: 34, Loss: 0.280303\n",
      "Epoch: 35, Loss: 0.759306\n",
      "Epoch: 36, Loss: 0.038348\n",
      "Epoch: 37, Loss: 0.068329\n",
      "Epoch: 38, Loss: 0.039986\n",
      "Epoch: 39, Loss: 0.035171\n",
      "Epoch: 40, Loss: 0.129103\n",
      "Epoch: 41, Loss: 0.073021\n",
      "Epoch: 42, Loss: 0.087566\n",
      "Epoch: 43, Loss: 0.075423\n",
      "Epoch: 44, Loss: 0.142138\n",
      "Epoch: 45, Loss: 0.052697\n",
      "Epoch: 46, Loss: 0.140854\n",
      "Epoch: 47, Loss: 2.476484\n",
      "Epoch: 48, Loss: 0.086888\n",
      "Epoch: 49, Loss: 0.020794\n",
      "Epoch: 50, Loss: 0.083370\n",
      "Epoch: 51, Loss: 0.077601\n",
      "Epoch: 52, Loss: 0.011000\n",
      "Epoch: 53, Loss: 0.018834\n",
      "Epoch: 54, Loss: 0.127095\n",
      "Epoch: 55, Loss: 0.110136\n",
      "Epoch: 56, Loss: 0.100961\n",
      "Epoch: 57, Loss: 0.044808\n",
      "Epoch: 58, Loss: 0.043447\n",
      "Epoch: 59, Loss: 0.024886\n",
      "Epoch: 60, Loss: 0.045397\n",
      "Epoch: 61, Loss: 0.065147\n",
      "Epoch: 62, Loss: 0.015928\n",
      "Epoch: 63, Loss: 0.001956\n",
      "Epoch: 64, Loss: 0.021409\n",
      "Epoch: 65, Loss: 0.049017\n",
      "Epoch: 66, Loss: 0.004943\n",
      "Epoch: 67, Loss: 0.002762\n",
      "Epoch: 68, Loss: 0.002267\n",
      "Epoch: 69, Loss: 0.004266\n",
      "Epoch: 70, Loss: 0.007803\n",
      "Epoch: 71, Loss: 0.000706\n",
      "Epoch: 72, Loss: 0.011543\n",
      "Epoch: 73, Loss: 0.000934\n",
      "Epoch: 74, Loss: 0.010561\n",
      "Epoch: 75, Loss: 0.010788\n",
      "Epoch: 76, Loss: 0.025387\n",
      "Epoch: 77, Loss: 0.004365\n",
      "Epoch: 78, Loss: 0.005524\n",
      "Epoch: 79, Loss: 0.002600\n",
      "Epoch: 80, Loss: 0.008227\n",
      "Epoch: 81, Loss: 0.004769\n",
      "Epoch: 82, Loss: 0.756529\n",
      "Epoch: 83, Loss: 0.018368\n",
      "Epoch: 84, Loss: 0.008466\n",
      "Epoch: 85, Loss: 0.023590\n",
      "Epoch: 86, Loss: 0.014143\n",
      "Epoch: 87, Loss: 0.010498\n",
      "Epoch: 88, Loss: 0.000760\n",
      "Epoch: 89, Loss: 0.003222\n",
      "Epoch: 90, Loss: 0.001286\n",
      "Epoch: 91, Loss: 0.001640\n",
      "Epoch: 92, Loss: 0.000268\n",
      "Epoch: 93, Loss: 0.003064\n",
      "Epoch: 94, Loss: 0.012428\n",
      "Epoch: 95, Loss: 0.000974\n",
      "Epoch: 96, Loss: 0.001828\n",
      "Epoch: 97, Loss: 0.004211\n",
      "Epoch: 98, Loss: 0.000570\n",
      "Epoch: 99, Loss: 0.001419\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size=64, shuffle=True)\n",
    "\n",
    "model = nn.Sequential(nn.Linear(3072, 1024),\n",
    "                      nn.Tanh(),\n",
    "                      nn.Linear(1024, 512),\n",
    "                      nn.Tanh(),\n",
    "                      nn.Linear(512, 128),\n",
    "                      nn.Tanh(),\n",
    "                      nn.Linear(128, 2))\n",
    "\n",
    "learning_rate = 1e-2\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "n_epochs = 100\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for imgs, labels in train_loader:\n",
    "        batch_size = imgs.shape[0]\n",
    "        outputs = model(imgs.view(batch_size, -1))\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(\"Epoch: %d, Loss: %f\" % (epoch, float(loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.801000\n"
     ]
    }
   ],
   "source": [
    "val_loader = torch.utils.data.DataLoader(cifar2_val, batch_size=64, shuffle=False)\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in val_loader:\n",
    "        batch_size = imgs.shape[0]\n",
    "        outputs = model(imgs.view(batch_size, -1))\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        total += labels.shape[0]\n",
    "        correct += int((predicted == labels).sum())\n",
    "print(\"Accuracy: %f\" % (correct/total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 训练集比测试集的效果要高得多，说明出现了过拟合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.查看模型参数数量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> PyTorch的nn.Model的parameters()方法提供了一种快速确定模型有多少个参数的方法，该方法与我们向优化器提供参数的方法相同。要找出每个张量实例中有多少个元素，我们可以调用numel()方法，把各分量的元素加起来就得到张量元素的总数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'connected_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-73-f58983a09fc4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m numel_list = [p.numel()\n\u001b[1;32m----> 2\u001b[1;33m               \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mconnected_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m               if p.requires_grad == True]\n\u001b[0;32m      4\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumel_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumel_list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'connected_model' is not defined"
     ]
    }
   ],
   "source": [
    "numel_list = [p.numel()\n",
    "              for p in connected_model.parameters()\n",
    "              if p.requires_grad == True]\n",
    "sum(numel_list), numel_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "360.838px",
    "left": "1080px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
